{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12.04.Fri.Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOy7jcmGUvsbekX+bX8+kKC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gotjd709/AI_class_KSA/blob/main/12_04_Fri_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "sobqnlZRXH6y",
        "outputId": "bf64a29f-d24c-40a8-afa8-5d8761569652"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "X = tf.constant([1, 2, 3, 4, 5], dtype=tf.float32)\n",
        "Y = tf.constant([1, 2, 3, 4, 5], dtype=tf.float32)\n",
        "\n",
        "def loss(W):\n",
        "  H = W * X\n",
        "  cost = tf.reduce_mean(tf.square(H - Y))\n",
        "  return cost.numpy()\n",
        "\n",
        "W_val = []\n",
        "cost_val = []\n",
        "for w in np.arange(-3.0, 5.0, 0.1):\n",
        "  c = loss(w)\n",
        "  W_val.append(w)\n",
        "  cost_val.append(c)\n",
        "  if int(w*10) % 10 == 0:\n",
        "    print(f\"w = {w} \\t c = {c}\")\n",
        "\n",
        "plt.plot(W_val, cost_val)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w = -3.0 \t c = 176.0\n",
            "w = -2.099999999999999 \t c = 105.70999908447266\n",
            "w = -1.0999999999999983 \t c = 48.5099983215332\n",
            "w = -0.09999999999999742 \t c = 13.3100004196167\n",
            "w = 2.6645352591003757e-15 \t c = 11.0\n",
            "w = 1.0000000000000036 \t c = 0.0\n",
            "w = 2.0000000000000044 \t c = 11.0\n",
            "w = 3.0000000000000053 \t c = 44.0\n",
            "w = 4.000000000000006 \t c = 99.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSxoxMwss_3x"
      },
      "source": [
        "## Hypothesis(가설) 정의\n",
        "# data\n",
        "x_data = [1, 2, 3, 4, 5]\n",
        "y_data = [1, 2, 3, 4, 5]\n",
        "\n",
        "W = tf.Variable(tf.random.normal([1]), name='Weight')\n",
        "B = tf.Variable(tf.random.normal([1]), name='Bias')\n",
        "\n",
        "@tf.function\n",
        "def Hypothesis(X):\n",
        " return W * X + B"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0KJGz3OtRGo"
      },
      "source": [
        "## cost/loss 함수 정의\n",
        "@tf.function\n",
        "def loss(H, Y):\n",
        " return tf.reduce_mean(tf.square(H - Y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dk3_mIjitc-3"
      },
      "source": [
        "## train 함수 정의\n",
        "@tf.function\n",
        "def train(X, Y, learning_rate=0.01):\n",
        "  with tf.GradientTape() as tape:\n",
        "    _loss = loss(Hypothesis(X), Y)\n",
        "  _w, _b = tape.gradient(_loss, [W, B])\n",
        "  W.assign_sub(learning_rate * _w) # 텐서의 값 변경을 위한 메소드\n",
        "  B.assign_sub(learning_rate * _b) # assign(=), assign_add(+=), assign_sub(-=)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "U0wpRvpptvJ7",
        "outputId": "c90c5fce-7b7f-4e50-e721-0af8a84a43c1"
      },
      "source": [
        "## Training\n",
        "for step in range(2001):\n",
        "  train(x_data, y_data, learning_rate=0.01)\n",
        "  _c = loss(Hypothesis(x_data), y_data)\n",
        "  if step % 20 == 0:\n",
        "    print(f\"{step}: {_c.numpy()} {W.numpy()} {B.numpy()}\")  \n",
        "print('\\nfinal W =', W.numpy(), 'b =', B.numpy())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-12c8972448d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0m_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHypothesis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2826\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2828\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2829\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    <ipython-input-7-ce97b67aefe7>:5 train  *\n        _loss = loss(Hypothesis(X), Y)\n\n    TypeError: tf__loss() takes 1 positional argument but 2 were given\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "e8vxRNYivZI7",
        "outputId": "06fded24-07d8-46b5-9015-fbea39330dd9"
      },
      "source": [
        "for step in range(2001):\n",
        "  train(x_data, y_data, learning_rate=0.01)\n",
        "  _c = loss(Hypothesis(x_data), y_data)\n",
        "  if step % 20 == 0:\n",
        "    print(f\"{step}: {_c.numpy()} {W.numpy()} {B.numpy()}\")\n",
        "print('\\nfinal W =', W.numpy(), 'b =', B.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-e435d2983e07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0m_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHypothesis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{step}: {_c.numpy()} {W.numpy()} {B.numpy()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2826\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2828\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2829\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    <ipython-input-7-ce97b67aefe7>:5 train  *\n        _loss = loss(Hypothesis(X), Y)\n\n    TypeError: tf__loss() takes 1 positional argument but 2 were given\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4qRgQxdvoXI",
        "outputId": "ae568ded-f533-4968-98e8-8e9e8808d171"
      },
      "source": [
        "#test data\n",
        "test_data = [2, 4, 1, 5, 3]\n",
        "for data in test_data:\n",
        "  y = data * W.numpy() + B.numpy()\n",
        "  print(\"X =\", data, \"then Y =\", y)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X = 2 then Y = [-0.873053]\n",
            "X = 4 then Y = [-1.3215895]\n",
            "X = 1 then Y = [-0.64878476]\n",
            "X = 5 then Y = [-1.5458577]\n",
            "X = 3 then Y = [-1.0973213]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH2DvJpXvwmf",
        "outputId": "2c8766a8-e20f-49fc-c32d-b0c92ba38faf"
      },
      "source": [
        "import tensorflow as tf\n",
        "x_data = [1, 2, 3, 4, 5]\n",
        "y_data = [1, 2, 3, 4, 5]\n",
        "W = tf.Variable(tf.random.normal([1]), name='Weight')\n",
        "B = tf.Variable(tf.random.normal([1]), name='Bias')\n",
        "\n",
        "@tf.function\n",
        "def Hypothesis(X):\n",
        "  return W * X + B\n",
        "\n",
        "@tf.function\n",
        "def loss(H, Y):\n",
        "  return tf.reduce_mean(tf.square(H - Y))\n",
        "\n",
        "@tf.function\n",
        "def train(X, Y, learning_rate=0.01):\n",
        "  with tf.GradientTape() as tape:\n",
        "    _loss = loss(Hypothesis(X), Y)\n",
        "  _w, _b = tape.gradient(_loss, [W, B])\n",
        "  W.assign_sub(learning_rate * _w) # 텐서의 값 변경을 위한 메소드\n",
        "  B.assign_sub(learning_rate * _b) # assign(=), assign_add(+=), assign_sub(-=)\n",
        "\n",
        "for step in range(2001):\n",
        "  train(x_data, y_data, learning_rate=0.01)\n",
        "  _c = loss(Hypothesis(x_data), y_data)\n",
        "  if step % 20 == 0:\n",
        "    print(f\"{step}: {_c.numpy()} {W.numpy()} {B.numpy()}\")\n",
        "print('\\nfinal W =', W.numpy(), 'b =', B.numpy())\n",
        "\n",
        "#test data\n",
        "test_data = [2, 4, 1, 5, 3]\n",
        "for data in test_data:\n",
        "  y = data * W.numpy() + B.numpy()\n",
        "  print(\"X =\", data, \"then Y =\", y)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: 2.0953519344329834 [0.8582062] [-1.0081949]\n",
            "20: 0.1287611424922943 [1.2311627] [-0.84143674]\n",
            "40: 0.11241336911916733 [1.2176663] [-0.7858754]\n",
            "60: 0.09817145019769669 [1.2034186] [-0.73440576]\n",
            "80: 0.08573393523693085 [1.1900966] [-0.686309]\n",
            "100: 0.07487201690673828 [1.177647] [-0.64136195]\n",
            "120: 0.06538628041744232 [1.1660128] [-0.5993586]\n",
            "140: 0.057102322578430176 [1.1551404] [-0.56010616]\n",
            "160: 0.049867890775203705 [1.1449801] [-0.5234243]\n",
            "180: 0.04354998096823692 [1.1354852] [-0.48914477]\n",
            "200: 0.03803248703479767 [1.1266122] [-0.45711023]\n",
            "220: 0.03321407735347748 [1.1183202] [-0.42717364]\n",
            "240: 0.029006117954850197 [1.1105714] [-0.39919776]\n",
            "260: 0.02533123269677162 [1.1033299] [-0.37305406]\n",
            "280: 0.02212197706103325 [1.0965629] [-0.34862244]\n",
            "300: 0.01931926980614662 [1.0902388] [-0.32579085]\n",
            "320: 0.01687166467308998 [1.084329] [-0.3044546]\n",
            "340: 0.014734180644154549 [1.0788063] [-0.2845156]\n",
            "360: 0.012867447920143604 [1.0736451] [-0.26588243]\n",
            "380: 0.011237229220569134 [1.068822] [-0.2484695]\n",
            "400: 0.009813569486141205 [1.0643148] [-0.23219705]\n",
            "420: 0.008570253849029541 [1.0601028] [-0.2169903]\n",
            "440: 0.007484470494091511 [1.0561666] [-0.20277938]\n",
            "460: 0.006536233238875866 [1.0524882] [-0.1894992]\n",
            "480: 0.005708145909011364 [1.0490507] [-0.1770887]\n",
            "500: 0.004984965547919273 [1.0458384] [-0.16549106]\n",
            "520: 0.004353399388492107 [1.0428364] [-0.15465295]\n",
            "540: 0.0038018629420548677 [1.040031] [-0.14452459]\n",
            "560: 0.0033202017657458782 [1.0374093] [-0.13505954]\n",
            "580: 0.0028995450120419264 [1.0349593] [-0.12621434]\n",
            "600: 0.002532200189307332 [1.0326698] [-0.11794845]\n",
            "620: 0.0022113872691988945 [1.0305301] [-0.11022393]\n",
            "640: 0.0019312091171741486 [1.0285307] [-0.10300519]\n",
            "660: 0.001686552190221846 [1.0266622] [-0.09625927]\n",
            "680: 0.0014728704700246453 [1.0249162] [-0.08995517]\n",
            "700: 0.0012862719595432281 [1.0232843] [-0.08406393]\n",
            "720: 0.0011233130935579538 [1.0217595] [-0.07855855]\n",
            "740: 0.0009809895418584347 [1.0203344] [-0.07341367]\n",
            "760: 0.0008567089098505676 [1.0190027] [-0.0686057]\n",
            "780: 0.0007481729844585061 [1.0177583] [-0.06411268]\n",
            "800: 0.0006533822743222117 [1.0165952] [-0.05991389]\n",
            "820: 0.0005706046940758824 [1.0155084] [-0.05599006]\n",
            "840: 0.0004983142716810107 [1.0144928] [-0.05232323]\n",
            "860: 0.00043518218444660306 [1.0135436] [-0.04889655]\n",
            "880: 0.00038004625821486115 [1.0126566] [-0.04569426]\n",
            "900: 0.0003318959497846663 [1.0118277] [-0.04270172]\n",
            "920: 0.00028985028620809317 [1.0110531] [-0.0399052]\n",
            "940: 0.00025312777142971754 [1.0103294] [-0.03729191]\n",
            "960: 0.00022106005053501576 [1.0096527] [-0.03484971]\n",
            "980: 0.00019305269233882427 [1.0090206] [-0.0325673]\n",
            "1000: 0.000168591650435701 [1.0084298] [-0.0304344]\n",
            "1020: 0.00014723616186529398 [1.0078777] [-0.02844118]\n",
            "1040: 0.00012858201807830483 [1.0073618] [-0.02657855]\n",
            "1060: 0.00011228991206735373 [1.0068797] [-0.02483785]\n",
            "1080: 9.806521848076954e-05 [1.0064291] [-0.02321118]\n",
            "1100: 8.564128802390769e-05 [1.006008] [-0.02169108]\n",
            "1120: 7.479154010070488e-05 [1.0056146] [-0.02027054]\n",
            "1140: 6.531634426210076e-05 [1.005247] [-0.01894302]\n",
            "1160: 5.703987699234858e-05 [1.0049033] [-0.01770247]\n",
            "1180: 4.981485108146444e-05 [1.0045822] [-0.0165431]\n",
            "1200: 4.3502484913915396e-05 [1.004282] [-0.01545965]\n",
            "1220: 3.7990452256053686e-05 [1.0040016] [-0.01444714]\n",
            "1240: 3.3176711440319195e-05 [1.0037396] [-0.013501]\n",
            "1260: 2.8975187888136134e-05 [1.0034946] [-0.01261684]\n",
            "1280: 2.5303093934780918e-05 [1.0032657] [-0.01179054]\n",
            "1300: 2.209767626482062e-05 [1.0030519] [-0.01101837]\n",
            "1320: 1.929789868881926e-05 [1.0028521] [-0.01029676]\n",
            "1340: 1.6853360648383386e-05 [1.0026653] [-0.00962242]\n",
            "1360: 1.471839459554758e-05 [1.0024908] [-0.00899223]\n",
            "1380: 1.2853784028266091e-05 [1.0023277] [-0.00840335]\n",
            "1400: 1.1225280104554258e-05 [1.0021752] [-0.00785303]\n",
            "1420: 9.803366083360743e-06 [1.0020328] [-0.00733873]\n",
            "1440: 8.56093811307801e-06 [1.0018996] [-0.00685816]\n",
            "1460: 7.476568953279639e-06 [1.0017751] [-0.00640905]\n",
            "1480: 6.528920494019985e-06 [1.001659] [-0.00598932]\n",
            "1500: 5.702043381461408e-06 [1.0015503] [-0.0055971]\n",
            "1520: 4.97976452606963e-06 [1.0014489] [-0.00523056]\n",
            "1540: 4.349173650552984e-06 [1.001354] [-0.00488806]\n",
            "1560: 3.798094894591486e-06 [1.0012653] [-0.00456797]\n",
            "1580: 3.317296432214789e-06 [1.0011824] [-0.00426888]\n",
            "1600: 2.8967851903871633e-06 [1.001105] [-0.00398935]\n",
            "1620: 2.5301164896518458e-06 [1.0010327] [-0.00372812]\n",
            "1640: 2.2096389784564963e-06 [1.000965] [-0.00348399]\n",
            "1660: 1.929450718307635e-06 [1.0009018] [-0.00325588]\n",
            "1680: 1.6852958424351527e-06 [1.0008429] [-0.00304274]\n",
            "1700: 1.4719778391736327e-06 [1.0007876] [-0.00284358]\n",
            "1720: 1.2854638953285757e-06 [1.0007361] [-0.00265741]\n",
            "1740: 1.1224283298361115e-06 [1.0006878] [-0.00248337]\n",
            "1760: 9.80294657892955e-07 [1.0006429] [-0.00232078]\n",
            "1780: 8.563066558053833e-07 [1.0006008] [-0.00216886]\n",
            "1800: 7.478207635358558e-07 [1.0005616] [-0.0020269]\n",
            "1820: 6.529589313686301e-07 [1.0005248] [-0.00189426]\n",
            "1840: 5.704427508135268e-07 [1.0004904] [-0.00177025]\n",
            "1860: 4.982340442438726e-07 [1.0004584] [-0.00165436]\n",
            "1880: 4.351055906681722e-07 [1.0004283] [-0.00154606]\n",
            "1900: 3.7994664126017597e-07 [1.0004002] [-0.00144486]\n",
            "1920: 3.317820755910361e-07 [1.000374] [-0.00135027]\n",
            "1940: 2.8988225153625535e-07 [1.0003496] [-0.00126191]\n",
            "1960: 2.531340328459919e-07 [1.0003268] [-0.00117929]\n",
            "1980: 2.2112385522632394e-07 [1.0003052] [-0.00110215]\n",
            "2000: 1.9306260412577103e-07 [1.0002854] [-0.00102998]\n",
            "\n",
            "final W = [1.0002854] b = [-0.00102998]\n",
            "X = 2 then Y = [1.9995408]\n",
            "X = 4 then Y = [4.0001116]\n",
            "X = 1 then Y = [0.9992554]\n",
            "X = 5 then Y = [5.0003967]\n",
            "X = 3 then Y = [2.9998262]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E60CmOYnwRm0"
      },
      "source": [
        "x_data = tf.constant(\n",
        "  [[73, 80, 75],\n",
        "  [93, 88, 93],\n",
        "  [89, 91, 90],\n",
        "  [96, 98, 100],\n",
        "  [73, 66, 70]], dtype=tf.float32)\n",
        "y_data = tf.constant(\n",
        "  [[152],\n",
        "  [185],\n",
        "  [180],\n",
        "  [196],\n",
        "  [142]], dtype=tf.float32)\n",
        "\n",
        "W = tf.Variable(tf.random.normal([3, 1]), name='weight')\n",
        "B = tf.Variable(tf.random.normal([1]), name='bias')\n",
        "\n",
        "@tf.function\n",
        "def Hypothesis(X):\n",
        "  return tf.matmul(X, W) + B"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Qqwv7Hhwg2M"
      },
      "source": [
        "@tf.function\n",
        "def loss(H, Y):\n",
        "  return tf.reduce_mean(tf.square(H - Y))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW35WdNCwlNL"
      },
      "source": [
        "@tf.function\n",
        "def train(X, Y, learning_rate = 1e-5):\n",
        "  with tf.GradientTape() as tape:\n",
        "    _loss = loss(Hypothesis(X), Y)\n",
        "  _w, _b = tape.gradient(_loss, [W, B])\n",
        "  W.assign_sub(learning_rate * _w)\n",
        "  B.assign_sub(learning_rate * _b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqIkkaF8wlVq",
        "outputId": "fd5c8565-1c6f-4194-e568-091ecac9e386"
      },
      "source": [
        "for step in range(10001):\n",
        "  # _h = Hypothesis(x_data)\n",
        "  _c = loss(Hypothesis(x_data), y_data)\n",
        "  train(x_data, y_data, learning_rate=1e-5)\n",
        "  if step % 20 == 0:\n",
        "    print(f\"{step}: {_c.numpy()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: 90580.28125\n",
            "20: 126.65618896484375\n",
            "40: 125.31361389160156\n",
            "60: 123.98570251464844\n",
            "80: 122.67192077636719\n",
            "100: 121.3725357055664\n",
            "120: 120.08685302734375\n",
            "140: 118.81502532958984\n",
            "160: 117.5570297241211\n",
            "180: 116.31239318847656\n",
            "200: 115.08134460449219\n",
            "220: 113.86357116699219\n",
            "240: 112.65873718261719\n",
            "260: 111.46684265136719\n",
            "280: 110.28792572021484\n",
            "300: 109.1218032836914\n",
            "320: 107.96783447265625\n",
            "340: 106.82667541503906\n",
            "360: 105.6976547241211\n",
            "380: 104.5806884765625\n",
            "400: 103.47586822509766\n",
            "420: 102.38292694091797\n",
            "440: 101.30158996582031\n",
            "460: 100.23217010498047\n",
            "480: 99.1741943359375\n",
            "500: 98.12739562988281\n",
            "520: 97.0920181274414\n",
            "540: 96.06781768798828\n",
            "560: 95.05471801757812\n",
            "580: 94.05216217041016\n",
            "600: 93.06074523925781\n",
            "620: 92.07981872558594\n",
            "640: 91.10968780517578\n",
            "660: 90.1496353149414\n",
            "680: 89.20021057128906\n",
            "700: 88.26082611083984\n",
            "720: 87.33158874511719\n",
            "740: 86.41224670410156\n",
            "760: 85.5030288696289\n",
            "780: 84.60346984863281\n",
            "800: 83.71357727050781\n",
            "820: 82.8333740234375\n",
            "840: 81.96253967285156\n",
            "860: 81.10111999511719\n",
            "880: 80.24893188476562\n",
            "900: 79.40589904785156\n",
            "920: 78.57192993164062\n",
            "940: 77.74688720703125\n",
            "960: 76.93077087402344\n",
            "980: 76.12350463867188\n",
            "1000: 75.32483673095703\n",
            "1020: 74.5348129272461\n",
            "1040: 73.75325012207031\n",
            "1060: 72.9801254272461\n",
            "1080: 72.21533203125\n",
            "1100: 71.45857238769531\n",
            "1120: 70.71025085449219\n",
            "1140: 69.96974182128906\n",
            "1160: 69.23735809326172\n",
            "1180: 68.5126724243164\n",
            "1200: 67.7960433959961\n",
            "1220: 67.08695220947266\n",
            "1240: 66.38546752929688\n",
            "1260: 65.6915054321289\n",
            "1280: 65.00514221191406\n",
            "1300: 64.32598876953125\n",
            "1320: 63.65443801879883\n",
            "1340: 62.989723205566406\n",
            "1360: 62.3322639465332\n",
            "1380: 61.68195343017578\n",
            "1400: 61.03858184814453\n",
            "1420: 60.40227508544922\n",
            "1440: 59.7725715637207\n",
            "1460: 59.14979934692383\n",
            "1480: 58.53351593017578\n",
            "1500: 57.924163818359375\n",
            "1520: 57.32126998901367\n",
            "1540: 56.724754333496094\n",
            "1560: 56.13469696044922\n",
            "1580: 55.55084991455078\n",
            "1600: 54.973533630371094\n",
            "1620: 54.40228271484375\n",
            "1640: 53.8371696472168\n",
            "1660: 53.27812576293945\n",
            "1680: 52.72523880004883\n",
            "1700: 52.1781120300293\n",
            "1720: 51.636878967285156\n",
            "1740: 51.10154342651367\n",
            "1760: 50.571876525878906\n",
            "1780: 50.04795837402344\n",
            "1800: 49.529640197753906\n",
            "1820: 49.016929626464844\n",
            "1840: 48.50978469848633\n",
            "1860: 48.00790023803711\n",
            "1880: 47.51152420043945\n",
            "1900: 47.02046203613281\n",
            "1920: 46.5347785949707\n",
            "1940: 46.05411911010742\n",
            "1960: 45.57872772216797\n",
            "1980: 45.10853576660156\n",
            "2000: 44.643192291259766\n",
            "2020: 44.18303298950195\n",
            "2040: 43.727684020996094\n",
            "2060: 43.277408599853516\n",
            "2080: 42.831722259521484\n",
            "2100: 42.39091873168945\n",
            "2120: 41.954898834228516\n",
            "2140: 41.52355194091797\n",
            "2160: 41.09679412841797\n",
            "2180: 40.674705505371094\n",
            "2200: 40.25703048706055\n",
            "2220: 39.84394073486328\n",
            "2240: 39.43523025512695\n",
            "2260: 39.03084945678711\n",
            "2280: 38.630958557128906\n",
            "2300: 38.23522186279297\n",
            "2320: 37.84379577636719\n",
            "2340: 37.45661926269531\n",
            "2360: 37.07350540161133\n",
            "2380: 36.69458770751953\n",
            "2400: 36.31967544555664\n",
            "2420: 35.948753356933594\n",
            "2440: 35.581878662109375\n",
            "2460: 35.21894073486328\n",
            "2480: 34.85988235473633\n",
            "2500: 34.50463104248047\n",
            "2520: 34.1533088684082\n",
            "2540: 33.80565643310547\n",
            "2560: 33.461753845214844\n",
            "2580: 33.121551513671875\n",
            "2600: 32.78496551513672\n",
            "2620: 32.45201873779297\n",
            "2640: 32.1226921081543\n",
            "2660: 31.796777725219727\n",
            "2680: 31.47446060180664\n",
            "2700: 31.15555763244629\n",
            "2720: 30.84012794494629\n",
            "2740: 30.527942657470703\n",
            "2760: 30.21918296813965\n",
            "2780: 29.913837432861328\n",
            "2800: 29.611621856689453\n",
            "2820: 29.31270980834961\n",
            "2840: 29.016998291015625\n",
            "2860: 28.724407196044922\n",
            "2880: 28.435089111328125\n",
            "2900: 28.148696899414062\n",
            "2920: 27.86553382873535\n",
            "2940: 27.5853328704834\n",
            "2960: 27.3081111907959\n",
            "2980: 27.03385353088379\n",
            "3000: 26.762548446655273\n",
            "3020: 26.494186401367188\n",
            "3040: 26.228652954101562\n",
            "3060: 25.965984344482422\n",
            "3080: 25.706090927124023\n",
            "3100: 25.449081420898438\n",
            "3120: 25.19480323791504\n",
            "3140: 24.94319725036621\n",
            "3160: 24.69431495666504\n",
            "3180: 24.44808006286621\n",
            "3200: 24.204452514648438\n",
            "3220: 23.963407516479492\n",
            "3240: 23.725067138671875\n",
            "3260: 23.489229202270508\n",
            "3280: 23.255855560302734\n",
            "3300: 23.025127410888672\n",
            "3320: 22.796669006347656\n",
            "3340: 22.570682525634766\n",
            "3360: 22.347280502319336\n",
            "3380: 22.126205444335938\n",
            "3400: 21.907413482666016\n",
            "3420: 21.691038131713867\n",
            "3440: 21.476957321166992\n",
            "3460: 21.265117645263672\n",
            "3480: 21.0556640625\n",
            "3500: 20.848323822021484\n",
            "3520: 20.643301010131836\n",
            "3540: 20.44040298461914\n",
            "3560: 20.239696502685547\n",
            "3580: 20.041170120239258\n",
            "3600: 19.844722747802734\n",
            "3620: 19.650409698486328\n",
            "3640: 19.458101272583008\n",
            "3660: 19.267940521240234\n",
            "3680: 19.07975196838379\n",
            "3700: 18.89356231689453\n",
            "3720: 18.709369659423828\n",
            "3740: 18.527225494384766\n",
            "3760: 18.34702491760254\n",
            "3780: 18.168659210205078\n",
            "3800: 17.99220085144043\n",
            "3820: 17.817684173583984\n",
            "3840: 17.644975662231445\n",
            "3860: 17.47420883178711\n",
            "3880: 17.30518913269043\n",
            "3900: 17.138010025024414\n",
            "3920: 16.97258186340332\n",
            "3940: 16.808902740478516\n",
            "3960: 16.64702606201172\n",
            "3980: 16.486848831176758\n",
            "4000: 16.328384399414062\n",
            "4020: 16.171628952026367\n",
            "4040: 16.016494750976562\n",
            "4060: 15.8631010055542\n",
            "4080: 15.711283683776855\n",
            "4100: 15.561080932617188\n",
            "4120: 15.412483215332031\n",
            "4140: 15.26551628112793\n",
            "4160: 15.120038986206055\n",
            "4180: 14.976208686828613\n",
            "4200: 14.833816528320312\n",
            "4220: 14.693020820617676\n",
            "4240: 14.553651809692383\n",
            "4260: 14.415802001953125\n",
            "4280: 14.279397964477539\n",
            "4300: 14.144536018371582\n",
            "4320: 14.01105785369873\n",
            "4340: 13.878984451293945\n",
            "4360: 13.748324394226074\n",
            "4380: 13.619071960449219\n",
            "4400: 13.491205215454102\n",
            "4420: 13.364656448364258\n",
            "4440: 13.239524841308594\n",
            "4460: 13.1156587600708\n",
            "4480: 12.993158340454102\n",
            "4500: 12.871902465820312\n",
            "4520: 12.751996994018555\n",
            "4540: 12.63334846496582\n",
            "4560: 12.51594352722168\n",
            "4580: 12.399802207946777\n",
            "4600: 12.284887313842773\n",
            "4620: 12.17125415802002\n",
            "4640: 12.058744430541992\n",
            "4660: 11.947473526000977\n",
            "4680: 11.837376594543457\n",
            "4700: 11.728473663330078\n",
            "4720: 11.620674133300781\n",
            "4740: 11.514059066772461\n",
            "4760: 11.408552169799805\n",
            "4780: 11.304174423217773\n",
            "4800: 11.200904846191406\n",
            "4820: 11.098750114440918\n",
            "4840: 10.99770736694336\n",
            "4860: 10.89770221710205\n",
            "4880: 10.79869556427002\n",
            "4900: 10.700823783874512\n",
            "4920: 10.60395336151123\n",
            "4940: 10.508151054382324\n",
            "4960: 10.413324356079102\n",
            "4980: 10.319551467895508\n",
            "5000: 10.226754188537598\n",
            "5020: 10.13492202758789\n",
            "5040: 10.044089317321777\n",
            "5060: 9.954178810119629\n",
            "5080: 9.865254402160645\n",
            "5100: 9.777257919311523\n",
            "5120: 9.690150260925293\n",
            "5140: 9.604053497314453\n",
            "5160: 9.518807411193848\n",
            "5180: 9.434549331665039\n",
            "5200: 9.35106372833252\n",
            "5220: 9.268560409545898\n",
            "5240: 9.186861038208008\n",
            "5260: 9.106080055236816\n",
            "5280: 9.026094436645508\n",
            "5300: 8.947011947631836\n",
            "5320: 8.868725776672363\n",
            "5340: 8.791308403015137\n",
            "5360: 8.714668273925781\n",
            "5380: 8.638863563537598\n",
            "5400: 8.563852310180664\n",
            "5420: 8.489604949951172\n",
            "5440: 8.416205406188965\n",
            "5460: 8.343557357788086\n",
            "5480: 8.27165412902832\n",
            "5500: 8.200505256652832\n",
            "5520: 8.130130767822266\n",
            "5540: 8.060462951660156\n",
            "5560: 7.991588592529297\n",
            "5580: 7.9234299659729\n",
            "5600: 7.855950355529785\n",
            "5620: 7.7891974449157715\n",
            "5640: 7.7231855392456055\n",
            "5660: 7.657835483551025\n",
            "5680: 7.593208312988281\n",
            "5700: 7.529200553894043\n",
            "5720: 7.465884208679199\n",
            "5740: 7.4032487869262695\n",
            "5760: 7.341299533843994\n",
            "5780: 7.279989719390869\n",
            "5800: 7.21929407119751\n",
            "5820: 7.1592254638671875\n",
            "5840: 7.099842071533203\n",
            "5860: 7.041048526763916\n",
            "5880: 6.982884407043457\n",
            "5900: 6.925326347351074\n",
            "5920: 6.868395805358887\n",
            "5940: 6.812044620513916\n",
            "5960: 6.756295680999756\n",
            "5980: 6.701135158538818\n",
            "6000: 6.646529197692871\n",
            "6020: 6.592525482177734\n",
            "6040: 6.539057731628418\n",
            "6060: 6.486146450042725\n",
            "6080: 6.433812141418457\n",
            "6100: 6.382035255432129\n",
            "6120: 6.330796718597412\n",
            "6140: 6.280062198638916\n",
            "6160: 6.229866981506348\n",
            "6180: 6.180222511291504\n",
            "6200: 6.13107967376709\n",
            "6220: 6.082489490509033\n",
            "6240: 6.034377098083496\n",
            "6260: 5.98675012588501\n",
            "6280: 5.939659118652344\n",
            "6300: 5.89306640625\n",
            "6320: 5.846900939941406\n",
            "6340: 5.801266670227051\n",
            "6360: 5.756101131439209\n",
            "6380: 5.711375713348389\n",
            "6400: 5.667167663574219\n",
            "6420: 5.623406410217285\n",
            "6440: 5.580064296722412\n",
            "6460: 5.5371994972229\n",
            "6480: 5.494813442230225\n",
            "6500: 5.452834606170654\n",
            "6520: 5.411303997039795\n",
            "6540: 5.370184421539307\n",
            "6560: 5.329535961151123\n",
            "6580: 5.289301872253418\n",
            "6600: 5.2494120597839355\n",
            "6620: 5.210005283355713\n",
            "6640: 5.170981407165527\n",
            "6660: 5.132425785064697\n",
            "6680: 5.0941972732543945\n",
            "6700: 5.056412220001221\n",
            "6720: 5.01900053024292\n",
            "6740: 4.981949806213379\n",
            "6760: 4.945342540740967\n",
            "6780: 4.909084796905518\n",
            "6800: 4.873188495635986\n",
            "6820: 4.837665557861328\n",
            "6840: 4.802529335021973\n",
            "6860: 4.7677717208862305\n",
            "6880: 4.733345985412598\n",
            "6900: 4.699276924133301\n",
            "6920: 4.665562629699707\n",
            "6940: 4.632201671600342\n",
            "6960: 4.59916877746582\n",
            "6980: 4.566487789154053\n",
            "7000: 4.534161567687988\n",
            "7020: 4.502157688140869\n",
            "7040: 4.4704718589782715\n",
            "7060: 4.439105033874512\n",
            "7080: 4.408087730407715\n",
            "7100: 4.37738037109375\n",
            "7120: 4.346988201141357\n",
            "7140: 4.316950798034668\n",
            "7160: 4.287148475646973\n",
            "7180: 4.2577033042907715\n",
            "7200: 4.228524208068848\n",
            "7220: 4.199692249298096\n",
            "7240: 4.1711225509643555\n",
            "7260: 4.14285945892334\n",
            "7280: 4.114873886108398\n",
            "7300: 4.087189197540283\n",
            "7320: 4.0597686767578125\n",
            "7340: 4.032643795013428\n",
            "7360: 4.005821228027344\n",
            "7380: 3.979224443435669\n",
            "7400: 3.9529216289520264\n",
            "7420: 3.9268996715545654\n",
            "7440: 3.9011268615722656\n",
            "7460: 3.8756251335144043\n",
            "7480: 3.850379228591919\n",
            "7500: 3.825402021408081\n",
            "7520: 3.8006539344787598\n",
            "7540: 3.7761788368225098\n",
            "7560: 3.7519545555114746\n",
            "7580: 3.7279770374298096\n",
            "7600: 3.7042508125305176\n",
            "7620: 3.680752992630005\n",
            "7640: 3.657480239868164\n",
            "7660: 3.634481430053711\n",
            "7680: 3.6116702556610107\n",
            "7700: 3.5891194343566895\n",
            "7720: 3.5667977333068848\n",
            "7740: 3.5446929931640625\n",
            "7760: 3.5228171348571777\n",
            "7780: 3.50117826461792\n",
            "7800: 3.479732036590576\n",
            "7820: 3.4585061073303223\n",
            "7840: 3.437514543533325\n",
            "7860: 3.4167346954345703\n",
            "7880: 3.396116256713867\n",
            "7900: 3.375758409500122\n",
            "7920: 3.3555874824523926\n",
            "7940: 3.33561372756958\n",
            "7960: 3.3158626556396484\n",
            "7980: 3.29630970954895\n",
            "8000: 3.276921510696411\n",
            "8020: 3.25775408744812\n",
            "8040: 3.2387804985046387\n",
            "8060: 3.219982624053955\n",
            "8080: 3.201375961303711\n",
            "8100: 3.1829771995544434\n",
            "8120: 3.1647205352783203\n",
            "8140: 3.146682024002075\n",
            "8160: 3.1288084983825684\n",
            "8180: 3.111114740371704\n",
            "8200: 3.093611478805542\n",
            "8220: 3.076288938522339\n",
            "8240: 3.0590925216674805\n",
            "8260: 3.0420918464660645\n",
            "8280: 3.0253002643585205\n",
            "8300: 3.0086259841918945\n",
            "8320: 2.9921441078186035\n",
            "8340: 2.9758002758026123\n",
            "8360: 2.9596381187438965\n",
            "8380: 2.943615674972534\n",
            "8400: 2.927790641784668\n",
            "8420: 2.912100315093994\n",
            "8440: 2.896582841873169\n",
            "8460: 2.881199359893799\n",
            "8480: 2.865967035293579\n",
            "8500: 2.8508763313293457\n",
            "8520: 2.8359458446502686\n",
            "8540: 2.821164131164551\n",
            "8560: 2.806553363800049\n",
            "8580: 2.792045831680298\n",
            "8600: 2.7776927947998047\n",
            "8620: 2.7634613513946533\n",
            "8640: 2.749420404434204\n",
            "8660: 2.7354817390441895\n",
            "8680: 2.721681594848633\n",
            "8700: 2.7080256938934326\n",
            "8720: 2.694483995437622\n",
            "8740: 2.6811094284057617\n",
            "8760: 2.667819023132324\n",
            "8780: 2.654690742492676\n",
            "8800: 2.641695737838745\n",
            "8820: 2.6288161277770996\n",
            "8840: 2.6160693168640137\n",
            "8860: 2.603429079055786\n",
            "8880: 2.5909128189086914\n",
            "8900: 2.578526020050049\n",
            "8920: 2.5662755966186523\n",
            "8940: 2.5541176795959473\n",
            "8960: 2.5420804023742676\n",
            "8980: 2.530181407928467\n",
            "9000: 2.5183558464050293\n",
            "9020: 2.506685972213745\n",
            "9040: 2.495126247406006\n",
            "9060: 2.483640193939209\n",
            "9080: 2.472316265106201\n",
            "9100: 2.4610464572906494\n",
            "9120: 2.4499216079711914\n",
            "9140: 2.4388959407806396\n",
            "9160: 2.4279863834381104\n",
            "9180: 2.41717267036438\n",
            "9200: 2.4064559936523438\n",
            "9220: 2.3958356380462646\n",
            "9240: 2.385319232940674\n",
            "9260: 2.3749024868011475\n",
            "9280: 2.364576578140259\n",
            "9300: 2.3543810844421387\n",
            "9320: 2.344259738922119\n",
            "9340: 2.334230899810791\n",
            "9360: 2.324308395385742\n",
            "9380: 2.314462900161743\n",
            "9400: 2.3047451972961426\n",
            "9420: 2.2950851917266846\n",
            "9440: 2.2855324745178223\n",
            "9460: 2.27604079246521\n",
            "9480: 2.2666983604431152\n",
            "9500: 2.257391929626465\n",
            "9520: 2.2481980323791504\n",
            "9540: 2.2390809059143066\n",
            "9560: 2.230045795440674\n",
            "9580: 2.2210793495178223\n",
            "9600: 2.212233066558838\n",
            "9620: 2.203433036804199\n",
            "9640: 2.194732189178467\n",
            "9660: 2.1861047744750977\n",
            "9680: 2.1775641441345215\n",
            "9700: 2.169132947921753\n",
            "9720: 2.1607277393341064\n",
            "9740: 2.152419090270996\n",
            "9760: 2.1441917419433594\n",
            "9780: 2.1360418796539307\n",
            "9800: 2.1279563903808594\n",
            "9820: 2.1199467182159424\n",
            "9840: 2.112034320831299\n",
            "9860: 2.1041781902313232\n",
            "9880: 2.0963964462280273\n",
            "9900: 2.088669538497925\n",
            "9920: 2.081035852432251\n",
            "9940: 2.0734386444091797\n",
            "9960: 2.065915822982788\n",
            "9980: 2.0585029125213623\n",
            "10000: 2.0511186122894287\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbZm21fUxYNN",
        "outputId": "00d24a1d-edb5-4522-fdd1-ab8a74b36a56"
      },
      "source": [
        "test_data = tf.constant(\n",
        "  [[ 73, 80, 75],\\\n",
        "   [ 93, 88, 93],\\\n",
        "   [ 89, 91, 90],\\\n",
        "   [ 96, 98, 100],\\\n",
        "   [ 73, 66, 70],\\\n",
        "   [100, 70, 101],\\\n",
        "   [ 60, 70, 110],\\\n",
        "   [ 90, 100, 80]], dtype=tf.float32)\n",
        "\n",
        "print(\"Testing...\")\n",
        "for data in test_data:\n",
        "  y = Hypothesis([data])\n",
        "  print(\"X =\", data.numpy(), \"thenY =\", y.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing...\n",
            "X = [73. 80. 75.] thenY = [[153.6765]]\n",
            "X = [93. 88. 93.] thenY = [[183.38791]]\n",
            "X = [89. 91. 90.] thenY = [[181.56741]]\n",
            "X = [ 96.  98. 100.] thenY = [[194.49985]]\n",
            "X = [73. 66. 70.] thenY = [[142.37036]]\n",
            "X = [100.  70. 101.] thenY = [[173.42746]]\n",
            "X = [ 60.  70. 110.] thenY = [[111.59732]]\n",
            "X = [ 90. 100.  80.] thenY = [[195.44955]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vOadshsyIFW"
      },
      "source": [
        "# training data\n",
        "x_data = tf.constant([[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]], dtype=tf.float32)\n",
        "y_data = tf.constant([[0], [0], [0], [1], [1], [1]], dtype=tf.float32)\n",
        "\n",
        "W = tf.Variable(tf.random.normal([2, 1]), name='weight')\n",
        "B = tf.Variable(tf.random.normal([1]), name='bias')\n",
        "# define hypothesis\n",
        "@tf.function\n",
        "def Hypothesis(X):\n",
        "  return tf.sigmoid(tf.matmul(X, W) + B)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOULxyYR5HRP"
      },
      "source": [
        "# define cost function\n",
        "@tf.function\n",
        "def loss(H, Y):\n",
        "  cost = -tf.reduce_mean(Y * tf.math.log(H) + (1 - Y) * tf.math.log(1 - H))\n",
        "  return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hK4www45M8t"
      },
      "source": [
        "# minimize the cost function\n",
        "@tf.function\n",
        "def train(X, Y, learning_rate=0.1):\n",
        "  with tf.GradientTape() as tape:\n",
        "    _loss = loss(Hypothesis(X), Y)\n",
        "  _w, _b = tape.gradient(_loss, [W, B])\n",
        "  W.assign_sub(learning_rate * _w)\n",
        "  B.assign_sub(learning_rate * _b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pNxd_yt5T6v",
        "outputId": "d80b4126-3d80-41b6-c23e-79ea9473ff70"
      },
      "source": [
        "# training...\n",
        "for step in range(10001):\n",
        "  _c = loss(Hypothesis(x_data), y_data)\n",
        "  train(x_data, y_data, learning_rate=0.1)\n",
        "  if step % 500 == 0:\n",
        "    print(f\"{step}: {_c.numpy()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: 1.858324408531189\n",
            "500: 0.25768956542015076\n",
            "1000: 0.15694180130958557\n",
            "1500: 0.11261191964149475\n",
            "2000: 0.08802592754364014\n",
            "2500: 0.07239822298288345\n",
            "3000: 0.06156863272190094\n",
            "3500: 0.053608108311891556\n",
            "4000: 0.04750162363052368\n",
            "4500: 0.04266385734081268\n",
            "5000: 0.03873360902070999\n",
            "5500: 0.035475268959999084\n",
            "6000: 0.032728854566812515\n",
            "6500: 0.030381590127944946\n",
            "7000: 0.02835177443921566\n",
            "7500: 0.026578621938824654\n",
            "8000: 0.025016071274876595\n",
            "8500: 0.023628495633602142\n",
            "9000: 0.02238778956234455\n",
            "9500: 0.021271809935569763\n",
            "10000: 0.020262431353330612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "Lx5UjlFA5e5E",
        "outputId": "f17a4971-269c-4756-f52d-b508e6cc5eea"
      },
      "source": [
        "# accuracy computation\n",
        "@tf.function\n",
        "def test(H, Y):\n",
        "  # True if H > 0.5 else False\n",
        "  predicted = tf.cast(H > 0.5, dtype=tf.float32)\n",
        "  accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y),\\ dtype=tf.float32))\n",
        "  return predicted, accuracy\n",
        "\n",
        "# report accuracy...\n",
        "print(\"\\nAccuracy...\")\n",
        "_h = Hypothesis(x_data)\n",
        "_p, _a = test(_h, y_data)\n",
        "print(\"Hypothesis =\\n\", _h.numpy())\n",
        "print(\"Predicted =\\n\", _p.numpy())\n",
        "print(\"\\nAccuracy =\", _a.numpy())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-bb47daab3b98>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y),\\ dtype=tf.float32))\u001b[0m\n\u001b[0m                                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4qu1Iaw86TZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}